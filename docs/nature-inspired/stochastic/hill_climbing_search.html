<html>
          <head>
            <title>Stochastic Hill Climbing | Clever Algorithms</title><link href="https://cleveralgorithms.com/nature-inspired/clever.css" media="screen" rel="stylesheet" type="text/css"/>
          <!-- support to display ruby source nicely -->
            <link href="https://cleveralgorithms.com/nature-inspired/prettify.css" type="text/css" rel="stylesheet"/>
            <script type="text/javascript" src="https://cleveralgorithms.com/nature-inspired/prettify.js"></script>
          </head><!-- Google tag (gtag.js) -->
          <script async src="https://www.googletagmanager.com/gtag/js?id=G-H2HN1W0V21"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-H2HN1W0V21');
          </script><!-- call to display ruby source nicely -->
        <body onload="prettyPrint()"><div class="container"><!-- Start Header -->
          <center>
          <h1>Clever Algorithms: Nature-Inspired Programming Recipes</h1>
          <em>A book by Jason Brownlee</em>

          <p>
            <a href="/">Home</a> |
            <a href="/nature-inspired/">Read Online</a> |
            <a href="https://amzn.to/4iKM9uc">Amazon</a> |
            <a href="https://www.goodreads.com/book/show/10321060-clever-algorithms">GoodReads</a> |
            <a href="https://www.google.com.au/books/edition/Clever_Algorithms/SESWXQphCUkC">Google Books</a> |
            <a href="https://raw.githubusercontent.com/Jason2Brownlee/CleverAlgorithms/master/release/clever_algorithms.pdf">PDF</a> (<a href="https://raw.githubusercontent.com/Jason2Brownlee/CleverAlgorithms/master/release/clever_algorithms-src.zip">code</a>) |
            <a href="https://github.com/Jason2Brownlee/CleverAlgorithms">GitHub</a>
          </p>

          </center>
          <hr/>
          <br/>
          <!-- End Header --><div class='breadcrumb'>
<a href='../index.html'>Table of Contents</a>
&gt;&gt;
<a href='../stochastic.html'>Stochastic Algorithms</a>
&gt;&gt;
<a href='hill_climbing_search.html'>Stochastic Hill Climbing</a>
</div>
<h1><a name='stochastic_hill_climbing'>Stochastic Hill Climbing</a></h1>
<p>
<em>Stochastic Hill Climbing, SHC, Random Hill Climbing, RHC, Random Mutation Hill Climbing, RMHC.</em>
</p>

<h2><a name='taxonomy'>Taxonomy</a></h2>
<p>
The Stochastic Hill Climbing algorithm is a Stochastic Optimization algorithm and is a Local Optimization algorithm (contrasted to Global Optimization). It is a direct search technique, as it does not require derivatives of the search space.
Stochastic Hill Climbing is an extension of deterministic hill climbing algorithms such as Simple Hill Climbing (first-best neighbor), Steepest-Ascent Hill Climbing (best neighbor), and a parent of approaches such as Parallel Hill Climbing and Random-Restart Hill Climbing.
</p>


<h2><a name='strategy'>Strategy</a></h2>
<p>
The strategy of the Stochastic Hill Climbing algorithm is iterate the process of randomly selecting a neighbor for a candidate solution and only accept it if it results in an improvement.
The strategy was proposed to address the limitations of deterministic hill climbing techniques that were likely to get stuck in local optima due to their greedy acceptance of neighboring moves.
</p>


<h2><a name='procedure'>Procedure</a></h2>
<p>
Algorithm (below) provides a pseudocode listing of the Stochastic Hill Climbing algorithm for minimizing a cost function, specifically the Random Mutation Hill Climbing algorithm described by Forrest and Mitchell applied to a maximization optimization problem  [<a href='#Forrest1993'>Forrest1993</a>].
</p>
<div class='pseudocode'>
<strong><strong><code>Input</code></strong></strong>: 
$Iter_{max}$, <code>ProblemSize</code>
<br />
<strong><strong><code>Output</code></strong></strong>: 
<code>Current</code>
<br />
<code>Current</code> $\leftarrow$ <code>RandomSolution</code>{<code>ProblemSize</code>}<br />
<strong><code>For</code></strong> ($iter_i \in$ $Iter_{max}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;<code>Candidate</code> $\leftarrow$ <code>RandomNeighbor</code>{<code>Current</code>}<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> (<code>Cost</code>{<code>Candidate</code>} $\geq$ <code>Cost</code>{<code>Current</code>})<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>Current</code> $\leftarrow$ <code>Candidate</code><br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
<strong><code>End</code></strong><br />
<strong><code>Return</code></strong> (<code>Current</code>)<br />
</div>
<div class='caption'>Pseudocode for Stochastic Hill Climbing.</div>



<h2><a name='heuristics'>Heuristics</a></h2>
<ul>
<li> Stochastic Hill Climbing was designed to be used in discrete domains with explicit neighbors such as combinatorial optimization (compared to continuous function optimization).</li>
<li> The algorithm's strategy may be applied to continuous domains by making use of a step-size to define candidate-solution neighbors (such as Localized Random Search and Fixed Step-Size Random Search).</li>
<li> Stochastic Hill Climbing is a local search technique (compared to global search) and may be used to refine a result after the execution of a global search algorithm.</li>
<li> Even though the technique uses a stochastic process, it can still get stuck in local optima.</li>
<li> Neighbors with better or equal cost should be accepted, allowing the technique to navigate across plateaus in the response surface.</li>
<li> The algorithm can be restarted and repeated a number of times after it converges to provide an improved result (called Multiple Restart Hill Climbing).</li>
<li> The procedure can be applied to multiple candidate solutions concurrently, allowing multiple algorithm runs to be performed at the same time (called Parallel Hill Climbing).</li>
</ul>


<h2><a name='code_listing'>Code Listing</a></h2>
<p>
Listing (below) provides an example of the Stochastic Hill Climbing algorithm implemented in the Ruby Programming Language, specifically the Random Mutation Hill Climbing algorithm described by Forrest and Mitchell  [<a href='#Forrest1993'>Forrest1993</a>].
The algorithm is executed for a fixed number of iterations and is applied to a binary string optimization problem called 'One Max'. The objective of this maximization problem is to prepare a string of all '1' bits, where the cost function only reports the number of bits in a given string.
<pre class='prettyprint lang-rb'>
def onemax(vector)
  return vector.inject(0.0){|sum, v| sum + ((v=="1") ? 1 : 0)}
end

def random_bitstring(num_bits)
  return Array.new(num_bits){|i| (rand&lt;0.5) ? "1" : "0"}
end

def random_neighbor(bitstring)
  mutant = Array.new(bitstring)
  pos = rand(bitstring.size)
  mutant[pos] = (mutant[pos]=='1') ? '0' : '1'
  return mutant
end

def search(max_iterations, num_bits)
  candidate = {}
  candidate[:vector] = random_bitstring(num_bits)
  candidate[:cost] = onemax(candidate[:vector])
  max_iterations.times do |iter|
    neighbor = {}
    neighbor[:vector] = random_neighbor(candidate[:vector])
    neighbor[:cost] = onemax(neighbor[:vector])
    candidate = neighbor if neighbor[:cost] &gt;= candidate[:cost]
    puts " &gt; iteration #{(iter+1)}, best=#{candidate[:cost]}"
    break if candidate[:cost] == num_bits
  end
  return candidate
end

if __FILE__ == $0
  # problem configuration
  num_bits = 64
  # algorithm configuration
  max_iterations = 1000
  # execute the algorithm
  best = search(max_iterations, num_bits)
  puts "Done. Best Solution: c=#{best[:cost]}, v=#{best[:vector].join}"
end
</pre>
<div class='caption'>Stochastic Hill Climbing in Ruby</div>
<div class='download_src'>Download: <a href='stochastic_hill_climbing.rb'>stochastic_hill_climbing.rb</a>.</div>
</p>


<h2><a name='references'>References</a></h2>

<h3><a name='primary_sources'>Primary Sources</a></h3>
<p>
Perhaps the most popular implementation of the Stochastic Hill Climbing algorithm is by Forrest and Mitchell, who proposed the Random Mutation Hill Climbing (RMHC) algorithm (with communication from Richard Palmer) in a study that investigated the behavior of the genetic algorithm on a deceptive class of (discrete) bit-string optimization problems called 'royal road' functions  [<a href='#Forrest1993'>Forrest1993</a>]. The RMHC was compared to two other hill climbing algorithms in addition to the genetic algorithm, specifically: the Steepest-Ascent Hill Climber, and the Next-Ascent Hill Climber. This study was then followed up by Mitchell and Holland  [<a href='#Mitchell1993'>Mitchell1993</a>].
</p>
<p>
Jules and Wattenberg were also early to consider stochastic hill climbing as an approach to compare to the genetic algorithm  [<a href='#Juels1994'>Juels1994</a>].
Skalak applied the RMHC algorithm to a single long bit-string that represented a number of prototype vectors for use in classification  [<a href='#Skalak1994'>Skalak1994</a>].
</p>


<h3><a name='learn_more'>Learn More</a></h3>
<p>
The Stochastic Hill Climbing algorithm is related to the genetic algorithm without crossover. Simplified version's of the approach are investigated for bit-string based optimization problems with the population size of the genetic algorithm reduced to one. The general technique has been investigated under the names Iterated Hillclimbing  [<a href='#Muhlenbein1991'>Muhlenbein1991</a>], ES(1+1,m,hc)  [<a href='#Muhlenbein1992'>Muhlenbein1992</a>], Random Bit Climber  [<a href='#Davis1991'>Davis1991</a>], and (1+1)-Genetic Algorithm  [<a href='#Back1993'>Back1993</a>]. This main difference between RMHC and ES(1+1) is that the latter uses a fixed probability of a mutation for each discrete element of a solution (meaning the neighborhood size is probabilistic), whereas RMHC will only stochastically modify one element.
</p>




<h2>Bibliography</h2>
<table>
 <tr valign="top">
 <td><a name='Back1993'>[Back1993]</a></td>
 <td>T. B\&amp;auml;ck, "<a href="http://scholar.google.com.au/scholar?q=Optimal+Mutation+Rates+in+Genetic+Search">Optimal Mutation Rates in Genetic Search</a>", in Proceedings of the Fifth International Conference on Genetic Algorithms, 1993.</td>
 </tr>
 <tr valign="top">
 <td><a name='Davis1991'>[Davis1991]</a></td>
 <td>L. Davis, "<a href="http://scholar.google.com.au/scholar?q=Bit-climbing,+representational+bias,+and+test+suite+design">Bit-climbing, representational bias, and test suite design</a>", in Proceedings of the fourth international conference on genetic algorithms, 1991.</td>
 </tr>
 <tr valign="top">
 <td><a name='Forrest1993'>[Forrest1993]</a></td>
 <td>S. Forrest and M. Mitchell, "<a href="http://scholar.google.com.au/scholar?q=Relative+building-block+fitness+and+the+building-block+hypothesis">Relative building-block fitness and the building-block hypothesis</a>", in Foundations of Genetic Algorithms 2, 1993.</td>
 </tr>
 <tr valign="top">
 <td><a name='Juels1994'>[Juels1994]</a></td>
 <td>A. Juels and M. Wattenberg, "<a href="http://scholar.google.com.au/scholar?q=Stochastic+hill+climbing+as+a+baseline+method+for+evaluating+genetic\n\talgorithms">Stochastic hill climbing as a baseline method for evaluating genetic\n\talgorithms</a>", University of California, Berkeley, 1994.</td>
 </tr>
 <tr valign="top">
 <td><a name='Mitchell1993'>[Mitchell1993]</a></td>
 <td>M. Mitchell and J. H. Holland, "<a href="http://scholar.google.com.au/scholar?q=When+Will+a+Genetic+Algorithm+Outperform+Hill+Climbing?">When Will a Genetic Algorithm Outperform Hill Climbing?</a>", in Proceedings of the 5th International Conference on Genetic Algorithms, 1993.</td>
 </tr>
 <tr valign="top">
 <td><a name='Muhlenbein1991'>[Muhlenbein1991]</a></td>
 <td>H. M\&amp;uuml;hlenbein, "<a href="http://scholar.google.com.au/scholar?q=Evolution+in+time+and+space+-+the+parallel+genetic+algorithm">Evolution in time and space - the parallel genetic algorithm</a>", in Foundations of Genetic Algorithms, 1991.</td>
 </tr>
 <tr valign="top">
 <td><a name='Muhlenbein1992'>[Muhlenbein1992]</a></td>
 <td>H. M\&amp;uuml;hlenbein, "<a href="http://scholar.google.com.au/scholar?q=How+Genetic+Algorithms+Really+Work:+I.+Mutation+and+Hillclimbing">How Genetic Algorithms Really Work: I. Mutation and Hillclimbing</a>", in Parallel Problem Solving from Nature 2, 1992.</td>
 </tr>
 <tr valign="top">
 <td><a name='Skalak1994'>[Skalak1994]</a></td>
 <td>D. B. Skalak, "<a href="http://scholar.google.com.au/scholar?q=Prototype+and+Feature+Selection+by+Sampling+and+Random+Mutation+Hill\n\tClimbing+Algorithms">Prototype and Feature Selection by Sampling and Random Mutation Hill\n\tClimbing Algorithms</a>", in Proceedings of the eleventh international conference on machine learning, 1994.</td>
 </tr>
</table>

<br><br><br>
      <p><small>Please Note: This content was automatically generated from the book content and may contain minor differences.</small></p>
      <!-- generate math images from latex math snippets -->
    <script type="text/javascript" src="https://mathcache.s3.amazonaws.com/replacemath.js"></script>
    <script type="text/javascript">replaceMath(document.body);</script>
        </div></body>
          </html>