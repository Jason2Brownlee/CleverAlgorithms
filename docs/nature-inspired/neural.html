<html>
          <head>
            <title>Neural Algorithms | Clever Algorithms</title><link href="https://cleveralgorithms.com/nature-inspired/clever.css" media="screen" rel="stylesheet" type="text/css"/>
          <!-- support to display ruby source nicely -->
            <link href="https://cleveralgorithms.com/nature-inspired/prettify.css" type="text/css" rel="stylesheet"/>
            <script type="text/javascript" src="https://cleveralgorithms.com/nature-inspired/prettify.js"></script>
          </head><!-- Google tag (gtag.js) -->
          <script async src="https://www.googletagmanager.com/gtag/js?id=G-H2HN1W0V21"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-H2HN1W0V21');
          </script><!-- call to display ruby source nicely -->
        <body onload="prettyPrint()"><div class="container"><!-- Start Header -->
          <center>
          <h1>Clever Algorithms: Nature-Inspired Programming Recipes</h1>
          <em>A book by Jason Brownlee</em>

          <p>
            <a href="/">Home</a> |
            <a href="/nature-inspired/">Read Online</a> |
            <a href="https://amzn.to/4iKM9uc">Amazon</a> |
            <a href="https://www.goodreads.com/book/show/10321060-clever-algorithms">GoodReads</a> |
            <a href="https://www.google.com.au/books/edition/Clever_Algorithms/SESWXQphCUkC">Google Books</a> |
            <a href="https://raw.githubusercontent.com/Jason2Brownlee/CleverAlgorithms/master/release/clever_algorithms.pdf">PDF</a> (<a href="https://raw.githubusercontent.com/Jason2Brownlee/CleverAlgorithms/master/release/clever_algorithms-src.zip">code</a>) |
            <a href="https://github.com/Jason2Brownlee/CleverAlgorithms">GitHub</a>
          </p>

          </center>
          <hr/>
          <br/>
          <!-- End Header --><div class='breadcrumb'>
<a href='index.html'>Table of Contents</a>
&gt;&gt;
<a href='neural.html'>Neural Algorithms</a>
</div>
<h1><a name='neural_algorithms'>Neural Algorithms</a></h1>

<h2><a name='overview'>Overview</a></h2>
<p>
This chapter describes Neural Algorithms.
</p>

<h3><a name='biological_neural_networks'>Biological Neural Networks</a></h3>
<p>
A Biological Neural Network refers to the information processing elements of the nervous system, organized as a collection of neural cells, called neurons, that are interconnected in networks and interact with each other using electrochemical signals.
A biological neuron is generally comprised of dendrites which provide the input signals to the cell. The neuron reacts to input signals and may produce an output signal on its output connection called the axon which is connected to the dendrites of other neurons via synapses.
</p>
<p>
The study of biological neural networks falls within the domain of neuroscience which is a branch of biology concerned with the nervous system.
Neuroanatomy is a subject that is concerned with the the structure and function of groups of neural networks both with regard to parts of the brain and the structures that lead from and to the brain from the rest of the body.
Neuropsychology is another discipline concerned with the structure and function of the brain as they relate to abstract psychological behaviors.
For further information, refer to a good textbook on any of these general topics.
</p>


<h3><a name='artificial_neural_networks'>Artificial Neural Networks</a></h3>
<p>
The field of Artificial Neural Networks (ANN) is concerned with the investigation of computational models inspired by theories and observation of the structure and function of biological networks of neural cells in the brain. They are generally designed as models for addressing mathematical, computational, and engineering problems. As such, there is a lot of interdisciplinary research in mathematics, neurobiology and computer science.
</p>
<p>
An Artificial Neural Network is generally comprised of a collection of artificial neurons that are interconnected in order to performs some computation on input patterns and create output patterns. They are adaptive systems capable of modifying their internal structure, typically the weights between nodes in the network, allowing them to be used for a variety of function approximation problems such as classification, regression, feature extraction and content addressable memory.
</p>
<p>
Given that the focus of the field is on performing computation with networks of discrete computing units, the field is traditionally called a 'connectionist' paradigm of Artificial Intelligence and 'Neural Computation'.
</p>
<p>
There are many types of neural networks, many of which fall into one of two categories:
</p>
<ul>
<li> <strong>Feed-forward Networks</strong> where input is provided on one side of the network and the signals are propagated forward (in one direction) through the network structure to the other side where output signals are read. These networks may be comprised of one cell, one layer or multiple layers of neurons. Some examples include the Perceptron, Radial Basis Function Networks, and the multi-layer perceptron networks.</li>
<li> <strong>Recurrent Networks</strong> where cycles in the network are permitted and the structure may be fully interconnected. Examples include the Hopfield Network and Bidirectional Associative Memory.</li>
</ul>
<p>
Artificial Neural Network structures are made up of nodes and weights which typically require training based on samples of patterns from a problem domain. Some examples of learning strategies include:
</p>
<ul>
<li> <strong>Supervised Learning</strong> where the network is exposed to the input that has a known expected answer. The internal state of the network is modified to better match the expected result. Examples of this learning method include the Back-propagation algorithm and the Hebb rule.</li>
<li> <strong>Unsupervised Learning</strong> where the network is exposed to input patterns from which it must discern meaning and extract features. The most common type of unsupervised learning is competitive learning where neurons compete based on the input pattern to produce an output pattern. Examples include Neural Gas, Learning Vector Quantization, and the Self-Organizing Map.</li>
</ul>
<p>
Artificial Neural Networks are typically difficult to configure and slow to train, but once prepared are very fast in application. They are generally used for function approximation-based problem domains and prized for their capabilities of generalization and tolerance to noise. They are known to have the limitation of being opaque, meaning there is little explanation to the subject matter expert as to why decisions were made, only how.
</p>
<p>
There are many excellent reference texts for the field of Artificial Neural Networks, some selected texts include: &quot;<em>Neural Networks for Pattern Recognition</em>&quot; by Bishop  [<a href='#Bishop1995'>Bishop1995</a>], &quot;<em>Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks</em>&quot; by Reed and Marks II  [<a href='#Reed1999'>Reed1999</a>] and &quot;<em>An Introduction to Neural Networks</em>&quot; by Gurney  [<a href='#Gurney1997'>Gurney1997</a>].
</p>


<h3>Algorithms</h3>
<ul>
<li><a href='neural/perceptron.html'>Perceptron</a></li>
<li><a href='neural/backpropagation.html'>Back-propagation</a></li>
<li><a href='neural/hopfield_network.html'>Hopfield Network</a></li>
<li><a href='neural/lvq.html'>Learning Vector Quantization</a></li>
<li><a href='neural/som.html'>Self-Organizing Map</a></li>
</ul>
<h3><a name='extensions'>Extensions</a></h3>
<p>
There are many other algorithms and classes of algorithm that were not described from the field of Artificial Neural Networks, not limited to:
</p>
<ul>
<li> <strong>Radial Basis Function Network</strong>: A network where activation functions are controlled by Radial Basis Functions  [<a href='#Howlett2001'>Howlett2001</a>].</li>
<li> <strong>Neural Gas</strong>: Another self-organizing and unsupervised competitive learning algorithm. Unlike SOM (and more like LVQ), the nodes are not organized into a lower-dimensional structure, instead the competitive Hebbian-learning like rule is applied to connect, order, and adapt nodes in feature space  [<a href='#Martinetz1991'>Martinetz1991</a>] [<a href='#Martinetz1993'>Martinetz1993</a>] [<a href='#Martinetz1994'>Martinetz1994</a>].</li>
<li> <strong>Hierarchical Temporal Memory</strong>: A neural network system based on models of some of the structural and algorithmic properties of the neocortex  [<a href='#Hawkins2005'>Hawkins2005</a>].</li>
</ul>




<h3>Bibliography</h3>
<table>
 <tr valign="top">
 <td><a name='Bishop1995'>[Bishop1995]</a></td>
 <td>C. M. Bishop, "<a href="http://scholar.google.com.au/scholar?q=Neural+Networks+for+Pattern+Recognition">Neural Networks for Pattern Recognition</a>", Oxford University Press, 1995.</td>
 </tr>
 <tr valign="top">
 <td><a name='Gurney1997'>[Gurney1997]</a></td>
 <td>K. Gurney, "<a href="http://scholar.google.com.au/scholar?q=An+Introduction+to+Neural+Networks">An Introduction to Neural Networks</a>", CRC Press, 1997.</td>
 </tr>
 <tr valign="top">
 <td><a name='Hawkins2005'>[Hawkins2005]</a></td>
 <td>J. Hawkins and S. Blakeslee, "<a href="http://scholar.google.com.au/scholar?q=On+Intelligence">On Intelligence</a>", Henry Holt and Company, 2005.</td>
 </tr>
 <tr valign="top">
 <td><a name='Howlett2001'>[Howlett2001]</a></td>
 <td>Robert J. Howlett and L. C. Jain, "<a href="http://scholar.google.com.au/scholar?q=Radial+basis+function+networks+1:+recent+developments+in+theory+and\n\tapplications">Radial basis function networks 1: recent developments in theory and\n\tapplications</a>", Springer, 2001.</td>
 </tr>
 <tr valign="top">
 <td><a name='Martinetz1991'>[Martinetz1991]</a></td>
 <td>T. Martinetz and K. Schulten, "<a href="http://scholar.google.com.au/scholar?q=A+&quot;neural+gas&quot;+network+learns+topologies">A &quot;neural gas&quot; network learns topologies</a>", in Artificial Neural Networks, 1991.</td>
 </tr>
 <tr valign="top">
 <td><a name='Martinetz1993'>[Martinetz1993]</a></td>
 <td>T. M. Martinetz and S. G. Berkovich and K. J. Schulten, "<a href="http://scholar.google.com.au/scholar?q='Neural-gas'+network+for+vector+quantization+and+its+application\n\tto+time-series+prediction">'Neural-gas' network for vector quantization and its application\n\tto time-series prediction</a>", IEEE Transactions on Neural Networks, 1993.</td>
 </tr>
 <tr valign="top">
 <td><a name='Martinetz1994'>[Martinetz1994]</a></td>
 <td>T. Martinetz and K. Schulten, "<a href="http://scholar.google.com.au/scholar?q=Topology+representing+networks">Topology representing networks</a>", Neural Networks, 1994.</td>
 </tr>
 <tr valign="top">
 <td><a name='Reed1999'>[Reed1999]</a></td>
 <td>R. D. Reed and R. J. Marks II, "<a href="http://scholar.google.com.au/scholar?q=Neural+Smithing:+Supervised+Learning+in+Feedforward+Artificial+Neural\n\tNetworks">Neural Smithing: Supervised Learning in Feedforward Artificial Neural\n\tNetworks</a>", Mit Press, 1999.</td>
 </tr>
</table>

<br><br><br>
      <p><small>Please Note: This content was automatically generated from the book content and may contain minor differences.</small></p>
      <!-- generate math images from latex math snippets -->
    <script type="text/javascript" src="https://mathcache.s3.amazonaws.com/replacemath.js"></script>
    <script type="text/javascript">replaceMath(document.body);</script>
        </div></body>
          </html>